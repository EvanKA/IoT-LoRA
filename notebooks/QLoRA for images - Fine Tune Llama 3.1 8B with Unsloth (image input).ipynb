{"cells":[{"cell_type":"markdown","metadata":{"id":"aTaDCGTe78bK"},"source":["# Fine-tune Llama 3.1 8B with Unsloth\n","> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65939,"status":"ok","timestamp":1726552273070,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"},"user_tz":-420},"id":"PoPKQjga6obN","outputId":"57df0dc7-e180-4b06-f38d-3c2d9ebf2353"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]}],"source":["!pip install -qqqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n","\n","# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n","from torch import __version__; from packaging.version import Version as V\n","xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n","!pip install -qqqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off\n","\n","import torch\n","from trl import SFTTrainer\n","from datasets import load_dataset\n","from transformers import TrainingArguments, TextStreamer\n","from unsloth.chat_templates import get_chat_template\n","from unsloth import FastLanguageModel, is_bfloat16_supported"]},{"cell_type":"markdown","metadata":{"id":"matKaF-f-GiU"},"source":["## 1. Load model for PEFT"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":830,"status":"error","timestamp":1726552280778,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"},"user_tz":-420},"id":"zGX9wG7Lhc-z","outputId":"b234877f-de2d-4298-859b-80722c259082"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"Unsloth: llava-hf/llava-1.5-7b-hf not supported yet!\nMake an issue to https://github.com/unslothai/unsloth!","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-507b57e54504>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NouRed/Med-LLaVa-QLoRA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mdispatch_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastCohereModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise NotImplementedError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34mf\"Unsloth: {model_name} not supported yet!\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34m\"Make an issue to https://github.com/unslothai/unsloth!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Unsloth: llava-hf/llava-1.5-7b-hf not supported yet!\nMake an issue to https://github.com/unslothai/unsloth!"]}],"source":["# Load model\n","max_seq_length = 2048\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"NouRed/Med-LLaVa-QLoRA\",\n","    trust_remote_code=True,\n","    max_seq_length=max_seq_length,\n","    load_in_4bit=True,\n","    dtype=None,\n",")\n","\n","# Prepare model for PEFT\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n","    use_rslora=True,\n","    use_gradient_checkpointing=\"unsloth\"\n",")\n","print(model.print_trainable_parameters())"]},{"cell_type":"markdown","metadata":{"id":"hjDpwfjJ3RAL"},"source":["## 2. Prepare data and tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"aborted","timestamp":1726552275272,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"},"user_tz":-420},"id":"sqGnvaT8is-R"},"outputs":[],"source":["alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n","#dataset = load_dataset(\"fresvyx/alpaca-llama2-indonesian\", split = \"train\")\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","metadata":{"id":"zdfjufQd3XMi"},"source":["## 3. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1726552275272,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"},"user_tz":-420},"id":"gcPAQihcjcfl"},"outputs":[],"source":["trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 300,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1726552275272,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"},"user_tz":-420},"id":"y5CPBC3Puwwr"},"outputs":[],"source":["#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GiB.\")\n","print(f\"{start_gpu_memory} GiB of memory reserved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQMCtQByu4gb","executionInfo":{"status":"aborted","timestamp":1726552275273,"user_tz":-420,"elapsed":14,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMDfAI7nwszk","executionInfo":{"status":"aborted","timestamp":1726552275273,"user_tz":-420,"elapsed":14,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GiB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GiB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk7HZeXIb_Tr","executionInfo":{"status":"aborted","timestamp":1726552275274,"user_tz":-420,"elapsed":13,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["# prompt: In the current state, it'll generate an attributeerror due to model being none. What code (snippet) should be added to solve this error?\n","\n","# Add this code snippet before trainer.train()\n","# if hasattr(model, \"enable_gradient_checkpointing\"):\n","#    model.enable_gradient_checkpointing()"]},{"cell_type":"markdown","metadata":{"id":"CI_U9FHZ3ZLO"},"source":["## 4. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JXdjsLqkvZY","executionInfo":{"status":"aborted","timestamp":1726552275274,"user_tz":-420,"elapsed":13,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["# alpaca_prompt = Copied from above\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"You are an intelligent character counter\", # instruction\n","        \"How many r's are there in the word 'strawberry'? Please give the complete reasoning of your answer.\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I02n7xwcxQgQ","executionInfo":{"status":"aborted","timestamp":1726552275275,"user_tz":-420,"elapsed":14,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["# alpaca_prompt = Copied from above\n","'''\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Bilangan manakah yang lebih besar?\", # instruction\n","        \"9,11 atau 9,9, dan sebutkan alasannya.\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJ4_943MZyyo","executionInfo":{"status":"aborted","timestamp":1726552275275,"user_tz":-420,"elapsed":13,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["# alpaca_prompt = Copied from above\n","'''\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Bilangan manakah yang lebih besar? Petunjuk: Kurangkan bilangan kedua dari bilangan pertama, jika hasilnya negatif, maka bilangan kedua lebih kecil daripada bilangan pertama, dan sebaliknya.\", # instruction\n","        \"9,11 atau 9,9?\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n","tokenizer.batch_decode(outputs)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2feLMaEH2sR","executionInfo":{"status":"aborted","timestamp":1726552275276,"user_tz":-420,"elapsed":14,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["'''\n","FastLanguageModel.for_inference(model)\n","test_prompts = load_dataset(\"fka/awesome-chatgpt-prompts\", split = \"train\")\n","for prompt in test_prompts[\"prompt\"]:\n","  inputs = tokenizer(\n","  [\n","    alpaca_prompt.format(\n","        \"You are a bilingual assistant who can speak both English and Indonesian fluently\", # instruction\n","        prompt,\n","        \"\",\n","    )\n","  ], return_tensors = \"pt\").to(\"cuda\")\n","\n","  outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n","  print(tokenizer.batch_decode(outputs))\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImMGqzuteExe","executionInfo":{"status":"aborted","timestamp":1726552275276,"user_tz":-420,"elapsed":14,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["'''\n","# prompt: How to change the code in the cell below so that the output is prettified like a table (i.e. pretty dataframe format instead of ugly console format)?\n","\n","FastLanguageModel.for_inference(model)\n","test_prompts = load_dataset(\"fka/awesome-chatgpt-prompts\", split = \"train\")\n","results = []\n","for prompt in test_prompts[\"prompt\"]:\n","  inputs = tokenizer(\n","  [\n","    alpaca_prompt.format(\n","        \"You are a bilingual assistant who can speak both English and Indonesian fluently. Please respond in Indonesian if and only if the prompt is in Indonesian, respond in English otherwise.\", # instruction\n","        prompt,\n","        \"\",\n","    )\n","  ], return_tensors = \"pt\").to(\"cuda\")\n","\n","  outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n","  results.append(tokenizer.batch_decode(outputs))\n","\n","import pandas as pd\n","df = pd.DataFrame([result[0].split(\"\\n\\n###\") for result in results])\n","df.iloc[:, :4]\n","'''\n"]},{"cell_type":"markdown","metadata":{"id":"HunPZjPp3aWe"},"source":["## 5. Save trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORa-rPvGmT9p","executionInfo":{"status":"aborted","timestamp":1726552275276,"user_tz":-420,"elapsed":13,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["df.to_csv(\"test_prompt_results.csv\")\n","model.save_pretrained(\"lora_model\") # Local saving\n","tokenizer.save_pretrained(\"lora_model\")\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzcUOyksmgWH","executionInfo":{"status":"aborted","timestamp":1726552275276,"user_tz":-420,"elapsed":13,"user":{"displayName":"Evan Kurnia Alim","userId":"06904118571109037203"}}},"outputs":[],"source":["# model.save_pretrained_gguf(\"model\", tokenizer, \"q8_0\")\n","# quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n","# for quant in quant_methods:\n","#    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}